import re
import os

SENT_END_CHARS = set(".?!ã€‚ï¼Ÿï¼")

def format_timestamp(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    ms = int(round((seconds - int(seconds)) * 1000))
    return f"{h:02}:{m:02}:{s:02},{ms:03}"

def parse_timestamp(ts: str) -> float:
    h, m, rest = ts.split(":")
    s, ms = rest.split(",")
    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000

def read_srt(path: str):
    with open(path, "r", encoding="utf-8") as f:
        content = f.read().strip()
    blocks = re.split(r"\n\s*\n", content)
    entries = []
    for block in blocks:
        lines = block.strip().splitlines()
        if len(lines) >= 3:
            idx = lines[0].strip()
            times = lines[1].split("-->")
            start = parse_timestamp(times[0].strip())
            end = parse_timestamp(times[1].strip())
            text = " ".join(lines[2:]).strip()
            entries.append((start, end, text))
    return entries

def write_srt(entries, path: str):
    with open(path, "w", encoding="utf-8") as f:
        for i, (start, end, text) in enumerate(entries, start=1):
            f.write(f"{i}\n")
            f.write(f"{format_timestamp(start)} --> {format_timestamp(end)}\n")
            f.write(text + "\n\n")

def split_sentence_with_time(start, end, text):
    """æŠŠä¸€å¥é‡Œå¤šä¸ªç»“æŸç¬¦æ‹†æˆå¤šå¥ï¼Œå¹¶æŒ‰å•è¯æ•°åˆ†é…æ—¶é—´"""
    parts = re.split(r"([.?!ã€‚ï¼Ÿï¼])", text)  # æŒ‰ç»“æŸç¬¦åˆ‡åˆ†
    chunks = []
    buffer = ""
    for p in parts:
        if not p.strip():
            continue
        buffer += p
        if p in SENT_END_CHARS:  # ç¢°åˆ°ç»“æŸç¬¦å°±è¾“å‡º
            chunks.append(buffer.strip())
            buffer = ""
    if buffer:  # å¦‚æœæœ€åè¿˜æœ‰æ®‹ä½™ï¼ˆæ²¡æœ‰ç»“æŸç¬¦çš„ï¼‰
        chunks.append(buffer.strip())

    # æ—¶é—´åˆ†é…
    total_duration = end - start
    total_words = sum(len(c.split()) for c in chunks) or 1
    avg_word_time = total_duration / total_words

    result = []
    cur_start = start
    for c in chunks:
        word_count = len(c.split()) or 1
        cur_end = cur_start + word_count * avg_word_time
        result.append((cur_start, cur_end, c))
        cur_start = cur_end
    return result

def merge_sentences(entries):
    merged = []
    cache = []
    for start, end, text in entries:
        cache.append((start, end, text))
        if text and text[-1] in SENT_END_CHARS:
            new_start = cache[0][0]
            new_end = cache[-1][1]
            new_text = " ".join([t for _, _, t in cache])
            merged.extend(split_sentence_with_time(new_start, new_end, new_text))
            cache = []
    if cache:
        new_start = cache[0][0]
        new_end = cache[-1][1]
        new_text = " ".join([t for _, _, t in cache])
        merged.append((new_start, new_end, new_text))
    return merged

def process_file(file_path: str):
    if not file_path.lower().endswith(".srt"):
        return
    try:
        entries = read_srt(file_path)
        merged_entries = merge_sentences(entries)
        dir_name = os.path.dirname(file_path)
        base, ext = os.path.splitext(os.path.basename(file_path))
        output_path = os.path.join(dir_name, f"{base}-Merge{ext}")
        write_srt(merged_entries, output_path)
        print(f"âœ… å¤„ç†å®Œæˆ: {output_path}")
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥ {file_path}: {e}")

def process_path(path: str):
    if os.path.isfile(path):
        process_file(path)
    elif os.path.isdir(path):
        for root, _, files in os.walk(path):
            for name in files:
                process_file(os.path.join(root, name))
    else:
        print(f"âš ï¸ è¾“å…¥è·¯å¾„æ— æ•ˆ: {path}")

if __name__ == "__main__":
    # ğŸ‘‰ ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
    input_path = "/Users/jiangsai/Downloads/[English (auto-generated)] PTS M1 6ã€A Beginners Guide to Trading Psychology [DownSub.com].srt"
    process_path(input_path)

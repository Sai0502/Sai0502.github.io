# import os
# import re
# import time
# from zhconv import convert
# from faster_whisper import WhisperModel

# SENT_END_CHARS = ".?!ã€‚ï¼Ÿï¼"

# # =============== å·¥å…·å‡½æ•° ===============

# def format_timestamp(seconds: float) -> str:
#     h = int(seconds // 3600)
#     m = int((seconds % 3600) // 60)
#     s = int(seconds % 60)
#     ms = int(round((seconds - int(seconds)) * 1000))
#     return f"{h:02}:{m:02}:{s:02},{ms:03}"

# def split_by_punctuation(text: str):
#     parts = re.split(f"([{SENT_END_CHARS}])", text)
#     result, buffer = [], ""
#     for p in parts:
#         if not p.strip():
#             continue
#         buffer += p
#         if p in SENT_END_CHARS:
#             result.append(buffer.strip())
#             buffer = ""
#     if buffer:
#         result.append(buffer.strip())
#     return result

# def refine_sentences(entries):
#     global SPLIT_MULTI_END   # â¬…ï¸ å‘Šè¯‰ Python ç”¨å…¨å±€å¼€å…³
#     merged, cache = [], []

#     for start, end, text in entries:
#         cache.append((start, end, text))
#         full_text = " ".join([t for _, _, t in cache])

#         if full_text and full_text[-1] in SENT_END_CHARS:
#             new_start, new_end = cache[0][0], cache[-1][1]

#             if SPLIT_MULTI_END:
#                 sub_sentences = split_by_punctuation(full_text)
#                 duration = new_end - new_start
#                 total_words = sum(len(s.split()) for s in sub_sentences) or 1
#                 avg_word_time = duration / total_words

#                 cur_start = new_start
#                 for sub in sub_sentences:
#                     word_count = len(sub.split()) or 1
#                     cur_end = cur_start + word_count * avg_word_time
#                     merged.append((cur_start, cur_end, sub))
#                     cur_start = cur_end
#             else:
#                 merged.append((new_start, new_end, full_text))
#             cache = []

#     if cache:
#         new_start, new_end = cache[0][0], cache[-1][1]
#         merged.append((new_start, new_end, " ".join([t for _, _, t in cache])))

#     return merged

# def write_srt(entries, path: str):
#     with open(path, "w", encoding="utf-8") as f:
#         for i, (start, end, text) in enumerate(entries, start=1):
#             f.write(f"{i}\n")
#             f.write(f"{format_timestamp(start)} --> {format_timestamp(end)}\n")
#             f.write(text + "\n\n")

# # =============== è½¬å½•é€»è¾‘ ===============

# def transcribe_audio_file(model, input_path, mode):
#     start_time = time.time()
#     language = "en" if mode == "en" else "zh"

#     segments, info = model.transcribe(input_path, language=language)

#     base_dir = os.path.dirname(input_path)
#     base_name = os.path.splitext(os.path.basename(input_path))[0]

#     if mode == "en":
#         srt_path = os.path.join(base_dir, base_name + ".srt")
#         raw_entries = [(float(seg.start), float(seg.end), (seg.text or "").strip())
#                        for seg in segments if (seg.text or "").strip()]
#         final_entries = refine_sentences(raw_entries)
#         write_srt(final_entries, srt_path)
#         print(f"âœ… è‹±æ–‡å­—å¹•å·²ç”Ÿæˆ: {srt_path}")

#     elif mode == "zh":
#         txt_path = os.path.join(base_dir, base_name + ".txt")
#         full_text = "\n".join((seg.text or "").strip() for seg in segments)
#         simplified_text = convert(full_text, "zh-cn")
#         with open(txt_path, "w", encoding="utf-8") as txt_file:
#             txt_file.write(simplified_text)
#         print(f"âœ… ä¸­æ–‡å­—å¹•å·²ç”Ÿæˆ: {txt_path}")

#     elapsed_minutes = (time.time() - start_time) / 60
#     rest_seconds = (int(elapsed_minutes // 10) + 1) * 60
#     print(f"{input_path} è½¬å½•å®Œæˆï¼Œè€—æ—¶ {elapsed_minutes:.2f} åˆ†é’Ÿï¼Œä¼‘æ¯ {rest_seconds} ç§’ã€‚")
#     time.sleep(rest_seconds)

# def transcribe_directory(model, folder_path, mode):
#     for root, dirs, files in os.walk(folder_path):
#         dirs.sort()
#         files.sort()
#         for file in files:
#             if file.endswith((".mp3", ".m4a", ".webm", ".mp4", ".mkv", ".avi", ".wav", ".flac")):
#                 video_path = os.path.join(root, file)
#                 print(f"æ­£åœ¨è½¬å½•: {video_path}")
#                 transcribe_audio_file(model, video_path, mode)

# def cooking(input_path, whisper_model_name, mode):
#     model = WhisperModel(whisper_model_name, compute_type="int8")
#     if os.path.isdir(input_path):
#         transcribe_directory(model, input_path, mode)
#     elif os.path.isfile(input_path):
#         print(f"æ­£åœ¨è½¬å½•: {input_path}")
#         transcribe_audio_file(model, input_path, mode)
#     else:
#         print(f"âŒ æ— æ•ˆè·¯å¾„ï¼š{input_path}")

# # =============== ä¸»ç¨‹åºå…¥å£ï¼ˆé…ç½®é›†ä¸­åœ¨è¿™é‡Œï¼‰ ===============

# if __name__ == "__main__":
#     input_path = "/Users/jiangsai/Downloads/å¹»å½±äº¤æ˜“2024/Module 1 - Introduction to Phantom Trading"
#     whisper_model_name = "medium"  # "tiny", "base", "small", "medium", "large"
#     mode = "en"                    # "en" -> è‹±æ–‡SRT, "zh" -> ä¸­æ–‡TXT
#     SPLIT_MULTI_END = True         # True = æ‹†åˆ†å­å¥æŒ‰å¥å·åˆå¹¶ï¼Œ False =æ•´å¥è¾“å‡º

#     cooking(input_path, whisper_model_name, mode)
# pip install openai-whisper faster-whisper
#############################################################################
# import os



# import os
# import time

# def format_timestamp(seconds):
#     h = int(seconds // 3600)
#     m = int((seconds % 3600) // 60)
#     s = int(seconds % 60)
#     ms = int((seconds - int(seconds)) * 1000)
#     return f"{h:02}:{m:02}:{s:02},{ms:03}"

# def cpu_rest(start_time):
#     # CPUä¼‘æ¯ä¸€ä¸‹
#     elapsed_minutes = (time.time() - start_time) / 60
#     rest_seconds = (int(elapsed_minutes // 10) + 1) * 60
#     print(f"{input_path} è½¬å½•å®Œæˆï¼Œè€—æ—¶ {elapsed_minutes:.2f} åˆ†é’Ÿï¼Œä¼‘æ¯ {rest_seconds} ç§’ã€‚")
#     time.sleep(rest_seconds)

# def transcribe_openai_whisper(model, input_path, output_path, language="en"):
#     start_time = time.time()
#     result = model.transcribe(input_path, language=language)
#     srt_path = output_path.replace(".txt", ".srt")

#     with open(srt_path, "w", encoding="utf-8") as f:
#         for idx, segment in enumerate(result["segments"], start=1):
#             f.write(f"{idx}\n{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\n{segment['text'].strip()}\n\n")

#     print(f"[OpenAI-Whisper] âœ… {srt_path} ç”¨æ—¶ {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ")
#     # CPUä¼‘æ¯ä¸€ä¸‹
#     cpu_rest(start_time)

# def transcribe_faster_whisper(model, input_path, output_path, language="en"):
#     start_time = time.time()
#     segments, _ = model.transcribe(input_path, beam_size=5, language=language)
#     srt_path = output_path.replace(".txt", ".srt")

#     with open(srt_path, "w", encoding="utf-8") as f:
#         for idx, segment in enumerate(segments, start=1):
#             f.write(f"{idx}\n{format_timestamp(segment.start)} --> {format_timestamp(segment.end)}\n{segment.text.strip()}\n\n")

#     print(f"[Faster-Whisper] âœ… {srt_path} ç”¨æ—¶ {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ")
#     # CPUä¼‘æ¯ä¸€ä¸‹
#     cpu_rest(start_time)

# def transcribe_directory(transcribe_func, model, input_folder, language="en"):
#     files = [f for f in os.listdir(input_folder) if f.endswith((".mp3", ".m4a", ".webm", ".mp4", ".mkv", ".avi"))]
#     for f in files:
#         in_path = os.path.join(input_folder, f)
#         out_path = os.path.join(input_folder, os.path.splitext(f)[0] + ".txt")
#         print(f"ğŸ¬ {f} å¼€å§‹è½¬å½•...")
#         transcribe_func(model, in_path, out_path, language)

# def cooking(input_path, engine, model_name, device="cpu", language="en"):
#     if engine == "whisper":
#         import whisper
#         asr_model = whisper.load_model(model_name)
#         transcribe_func = transcribe_openai_whisper
#         model = asr_model

#     elif engine == "faster_whisper":
#         from faster_whisper import WhisperModel
#         compute_type = "int8" if device == "cpu" else "float16"
#         asr_model = WhisperModel(model_name, device=device, compute_type=compute_type)
#         transcribe_func = transcribe_faster_whisper
#         model = asr_model

#     else:
#         raise ValueError("âŒ engine å¿…é¡»æ˜¯ 'whisper' æˆ– 'faster_whisper'")

#     if os.path.isdir(input_path):
#         transcribe_directory(transcribe_func, model, input_path, language=language)
#     elif os.path.isfile(input_path):
#         out_path = os.path.splitext(input_path)[0] + ".txt"
#         print(f"ğŸ¬ {input_path} å¼€å§‹è½¬å½•...")
#         transcribe_func(model, input_path, out_path, language=language)
#     else:
#         print(f"âŒ æ— æ•ˆè·¯å¾„: {input_path}")

# if __name__ == "__main__":
#     input_path = "/Users/jiangsai/Downloads/å¹»å½±äº¤æ˜“2024/Module 1 - Introduction to Phantom Trading/PTS-M1_6ã€A Beginners Guide to Trading Psychology.mp4"
#     engine = "whisper"   # "whisper" æˆ– "faster_whisper"
#     model = "small"             # "tiny" / "base" / "small" / "medium" / "large-v2"
#     language = "en"             # "en" / "zh" / None(è‡ªåŠ¨è¯†åˆ«)

#     cooking(input_path, engine, model, device="cpu", language=language)

#############################################################################

import os
import time
import re

# =====================
# å·¥å…·å‡½æ•°
# =====================

SENT_END_CHARS = set(".?!ã€‚ï¼Ÿï¼")

def format_timestamp(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    ms = int(round((seconds - int(seconds)) * 1000))
    return f"{h:02}:{m:02}:{s:02},{ms:03}"

def parse_timestamp(ts: str) -> float:
    h, m, rest = ts.split(":")
    s, ms = rest.split(",")
    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000

# =====================
# Whisper è½¬å½•
# =====================

def cpu_rest(start_time):
    elapsed_minutes = (time.time() - start_time) / 60
    rest_seconds = (int(elapsed_minutes // 10) + 1) * 60
    print(f"è½¬å½•å®Œæˆï¼Œè€—æ—¶ {elapsed_minutes:.2f} åˆ†é’Ÿï¼Œä¼‘æ¯ {rest_seconds} ç§’ã€‚")
    time.sleep(rest_seconds)

def transcribe_openai_whisper(model, input_path, output_path, language="en", srt_txt="srt"):
    start_time = time.time()
    result = model.transcribe(input_path, language=language)

    srt_path = output_path.replace(".txt", ".srt")
    txt_path = output_path.replace(".txt", ".txt")

    if srt_txt == "srt":
        with open(srt_path, "w", encoding="utf-8") as f:
            for idx, segment in enumerate(result["segments"], start=1):
                f.write(f"{idx}\n{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\n{segment['text'].strip()}\n\n")
        print(f"[OpenAI-Whisper] âœ… {srt_path} ç”¨æ—¶ {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ")
        cpu_rest(start_time)
        return srt_path
    else:  # txt
        with open(txt_path, "w", encoding="utf-8") as f:
            for segment in result["segments"]:
                text = segment['text'].strip()
                if text:  # è¿‡æ»¤ç©ºè¡Œ
                    f.write(text + "\n")
        print(f"[OpenAI-Whisper] âœ… {txt_path} ç”¨æ—¶ {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ")
        cpu_rest(start_time)
        return txt_path
    
def transcribe_faster_whisper(model, input_path, output_path, language="en", srt_txt="srt"):
    start_time = time.time()
    segments, _ = model.transcribe(input_path, beam_size=5, language=language)

    srt_path = output_path.replace(".txt", ".srt")
    txt_path = output_path.replace(".txt", ".txt")

    if srt_txt == "srt":
        with open(srt_path, "w", encoding="utf-8") as f:
            for idx, segment in enumerate(segments, start=1):
                f.write(f"{idx}\n{format_timestamp(segment.start)} --> {format_timestamp(segment.end)}\n{segment.text.strip()}\n\n")
        print(f"[Faster-Whisper] âœ… {srt_path} ç”¨æ—¶ {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ")
        cpu_rest(start_time)
        return srt_path
    else:  # txt
        with open(txt_path, "w", encoding="utf-8") as f:
            for segment in segments:
                text = segment.text.strip()
                if text:  # è¿‡æ»¤ç©ºè¡Œ
                    f.write(text + "\n")
        print(f"[Faster-Whisper] âœ… {txt_path} ç”¨æ—¶ {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ")
        cpu_rest(start_time)
        return txt_path

# =====================
# SRT å¤„ç†
# =====================

def read_srt(path: str):
    with open(path, "r", encoding="utf-8") as f:
        content = f.read().strip()
    blocks = re.split(r"\n\s*\n", content)
    entries = []
    for block in blocks:
        lines = block.strip().splitlines()
        if len(lines) >= 3:
            times = lines[1].split("-->")
            start = parse_timestamp(times[0].strip())
            end = parse_timestamp(times[1].strip())
            text = " ".join(lines[2:]).strip()
            entries.append((start, end, text))
    return entries

def write_srt(entries, path: str):
    with open(path, "w", encoding="utf-8") as f:
        for i, (start, end, text) in enumerate(entries, start=1):
            f.write(f"{i}\n")
            f.write(f"{format_timestamp(start)} --> {format_timestamp(end)}\n")
            f.write(text + "\n\n")

def split_sentence_with_time(start, end, text):
    parts = re.split(r"([.?!ã€‚ï¼Ÿï¼])", text)
    chunks, buffer = [], ""
    for p in parts:
        if not p.strip():
            continue
        buffer += p
        if p in SENT_END_CHARS:
            chunks.append(buffer.strip())
            buffer = ""
    if buffer:
        chunks.append(buffer.strip())

    total_duration = end - start
    total_words = sum(len(c.split()) for c in chunks) or 1
    avg_word_time = total_duration / total_words

    result, cur_start = [], start
    for c in chunks:
        word_count = len(c.split()) or 1
        cur_end = cur_start + word_count * avg_word_time
        result.append((cur_start, cur_end, c))
        cur_start = cur_end
    return result

def merge_sentences(entries):
    merged, cache = [], []
    for start, end, text in entries:
        cache.append((start, end, text))
        if text and text[-1] in SENT_END_CHARS:
            new_start, new_end = cache[0][0], cache[-1][1]
            new_text = " ".join([t for _, _, t in cache])
            merged.extend(split_sentence_with_time(new_start, new_end, new_text))
            cache = []
    if cache:
        new_start, new_end = cache[0][0], cache[-1][1]
        new_text = " ".join([t for _, _, t in cache])
        merged.append((new_start, new_end, new_text))
    return merged

# def process_srt(file_path: str):
#     entries = read_srt(file_path)
#     merged_entries = merge_sentences(entries)
#     output_path = file_path.replace(".srt", "-Merge.srt")
#     write_srt(merged_entries, output_path)
#     print(f"âœ… æ‹†åˆ†å®Œæˆ: {output_path}")
def process_srt(file_path: str):
    entries = read_srt(file_path)
    merged_entries = merge_sentences(entries)

    # ====== æ–°é€»è¾‘ï¼šæŠŠåŸå§‹æ–‡ä»¶ç§»åˆ° original/ å­ç›®å½• ======
    base_dir = os.path.dirname(file_path)
    base_name = os.path.basename(file_path)
    original_dir = os.path.join(base_dir, "original")
    os.makedirs(original_dir, exist_ok=True)

    # ç§»åŠ¨åŸå§‹æ–‡ä»¶
    original_path = os.path.join(original_dir, base_name)
    os.replace(file_path, original_path)

    # æœ€ç»ˆå­—å¹•æ–‡ä»¶ï¼ˆå’Œè§†é¢‘åŒåï¼‰
    output_path = file_path  # ç›´æ¥è¦†ç›–åŸå§‹ä½ç½®
    write_srt(merged_entries, output_path)

    print(f"âœ… åŸå§‹å­—å¹•å·²ç§»åˆ°: {original_path}")
    print(f"âœ… æœ€ç»ˆå¤„ç†å­—å¹•: {output_path}")
# =====================
# ä¸»å…¥å£
# =====================

# def cooking(input_path, engine, model_name, device="cpu", language="en", split_segment=False):
#     if engine == "whisper":
#         import whisper
#         asr_model = whisper.load_model(model_name)
#         transcribe_func = transcribe_openai_whisper
#     elif engine == "faster_whisper":
#         from faster_whisper import WhisperModel
#         compute_type = "int8" if device == "cpu" else "float16"
#         asr_model = WhisperModel(model_name, device=device, compute_type=compute_type)
#         transcribe_func = transcribe_faster_whisper
#     else:
#         raise ValueError("âŒ engine å¿…é¡»æ˜¯ 'whisper' æˆ– 'faster_whisper'")

#     if os.path.isfile(input_path):
#         out_path = os.path.splitext(input_path)[0] + ".txt"
#         print(f"ğŸ¬ {input_path} å¼€å§‹è½¬å½•...")
#         srt_path = transcribe_func(asr_model, input_path, out_path, language)
#         if split_segment:
#             process_srt(srt_path)
#     elif os.path.isdir(input_path):
#         for f in os.listdir(input_path):
#             if f.endswith((".mp3", ".m4a", ".webm", ".mp4", ".mkv", ".avi")):
#                 in_path = os.path.join(input_path, f)
#                 out_path = os.path.splitext(in_path)[0] + ".txt"
#                 print(f"ğŸ¬ {f} å¼€å§‹è½¬å½•...")
#                 srt_path = transcribe_func(asr_model, in_path, out_path, language)
#                 if split_segment:
#                     process_srt(srt_path)
#     else:
#         print(f"âŒ æ— æ•ˆè·¯å¾„: {input_path}")

def cooking(input_path, engine, model_name, device="cpu", language="en", split_segment=False, srt_txt="srt"):
    if engine == "whisper":
        import whisper
        asr_model = whisper.load_model(model_name)
        transcribe_func = transcribe_openai_whisper
    elif engine == "faster_whisper":
        from faster_whisper import WhisperModel
        compute_type = "int8" if device == "cpu" else "float16"
        asr_model = WhisperModel(model_name, device=device, compute_type=compute_type)
        transcribe_func = transcribe_faster_whisper
    else:
        raise ValueError("âŒ engine å¿…é¡»æ˜¯ 'whisper' æˆ– 'faster_whisper'")

    if os.path.isfile(input_path):
        out_path = os.path.splitext(input_path)[0] + ".txt"
        print(f"ğŸ¬ {input_path} å¼€å§‹è½¬å½•...")
        srt_path = transcribe_func(asr_model, input_path, out_path, language, srt_txt=srt_txt)
        if split_segment and srt_txt == "srt":
            process_srt(srt_path)
    elif os.path.isdir(input_path):
        for f in os.listdir(input_path):
            if f.endswith((".mp3", ".m4a", ".webm", ".mp4", ".mkv", ".avi")):
                in_path = os.path.join(input_path, f)
                out_path = os.path.splitext(in_path)[0] + ".txt"
                print(f"ğŸ¬ {f} å¼€å§‹è½¬å½•...")
                srt_path = transcribe_func(asr_model, in_path, out_path, language, srt_txt=srt_txt)
                if split_segment and srt_txt == "srt":
                    process_srt(srt_path)
    else:
        print(f"âŒ æ— æ•ˆè·¯å¾„: {input_path}")


if __name__ == "__main__":
    input_path = "/Users/jiangsai/Downloads/å¹»å½±äº¤æ˜“2024/Module 2 - Candlestick & Swing Formation Basics/PTS-M2_4ã€Market Sessions.mp4"
    engine = "whisper"    # "whisper" æˆ– "faster_whisper"
    model = "small"       # "tiny" / "base" / "small" / "medium" / "large-v2"
    split_segment = True  # False = ç±»ä¼¼YouTubeé‚£ç§æ®µä¸æˆæ®µçš„å­—å¹• True = æ‹†åˆ†æŒ‰å¥å·åˆå¹¶
    srt_txt = "srt"       # "srt" = è¾“å‡ºå­—å¹•  "txt" = è¾“å‡ºçº¯æ–‡æœ¬
    language="en"
    cooking(input_path, engine, model, device="cpu", language=language, split_segment=split_segment, srt_txt=srt_txt)
